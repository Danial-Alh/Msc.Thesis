\chapter{پیاده‌سازی، آزمایش‌ها و ارزیابی}\label{Chap:Chap4}
\minitoc
\section{مقدمه}
تا به حال به لحاظ نظری به نحوه حل مسئله پرداخته شد. در این فصل پس از معرفی دادگان آموزشی و معیارهای ارزیابی مورد استفاده، نحوه پیاده‌سازی و دشواری‌های پیش‌آمده هنگام آموزش شرح داده خواهند شد. در انتها نیز ضمن گزارش نتایج، عملکرد مدل‌ها با یکدیگر مقایسه شده‌اند.
\section{معیارهای ارزیابی}
از آنجا که در مدل‌های با فضای نهان امکان محاسبه دقیق \likelihood{} وجود ندارد، این معیار از معیارهای ارزیابی حذف گردیده است. می‌توان به عنوان جایگزین از معیارهایی همچون \revperplexity{} استفاده نمود. این روش به این صورت است که یک مدل زبانی میانی با استفاده از نمونه‌های مدل آموزش داده شده و \likelihood{} نمونه‌های آموزشی در مدل آموزش داده شده اندازه‌گیری می‌شود. این معیار نیز حساس به کیفیت و تنوع است. اگر مدل میانی آموزش داده شده، چه به دلیل عدم تنوع  و یا عدم کیفیت نمونه‌های مدل مورد ارزیابی احتمال کمی به نمونه‌های آموزشی نسبت دهد، معیار حساس بوده و مدل را جریمه خواهد کرد. نکته قابل توجه این است که به دلیل اینکه معیار آموزش مدل میانی، بر اساس بیشینه کردن \likelihood{} است، این امکان وجود دارد تا مدل مورد آموزش رفتار \meanseeking{} از خود بروز داده و به نقاطی که نمونه آموزشی از آن‌ها وجود ندارد، احتمال بالایی نسبت دهد. اما این اتفاق زمانی رخ می‌دهد که ظرفیت مدل کمتر از پیچیدگی نمونه‌های آموزشی باشد؛ این در حالیست که این نمونه‌های آموزشی توسط مدل مشابهی با ظرفیت مشابه تولید شده اند و احتمالا چنین نگرانی‌ای وجود نخواهد داشت.
\section{دادگان آموزشی}
\section{آموزش \wae{}}
همان طور که در فصل ؟؟؟ توضیح داده شد، آموزش شبکه از دو بخش کلی تشکیل شده است. اولین بخش مربوط به آموزش \wae{} است. به این منظور به عنوان اولین تلاش از معماری \lr{LSTM} با فضای نهان ؟؟؟ بعدی در \encoder{} و \decoder{} با بردارهای  \embedding{} ؟؟؟ تایی استفاده شد. مشکل‌هایی در آموزش این معماری پیش آمد که در ادامه توضیح داده خواهند شد. لازم به ذکر است همان طور که در فصل ؟؟؟ توضیح داده شد، \decoder{} بایستی 
$G(Z) = \argmax_{x} P_\psi(x|Z)$
و فضای خروجی $P_\psi$ یک بردار $|V|^{m}$ بعدی است که $m$ حداکثر طول جملات و $|V|$ نیز اندازه واژگان است؛ اما از آنجا که توانایی محاسباتی یافتن $\argmax$ بر روی چنین فضای بزرگی را نداریم بنابراین از روش 
\trans{\greedydecoding}{Greedy decoding}
که تقریبی از $\argmax$ بر روی کل فضاست استفاده می‌کنیم.
\iffalse
\subsection{\encoder{}،
عامل آموزش ناموفق}
پس از آموزش شبکه توضیح داده شده، معضل عدم توجه \decoder{} به فضای نهان پیش آمد. تصاویر ؟؟؟ مربوط به نتایج \bleu{} و \selfbleu{} در دو حالت گزارش شده است. در حالت اول یک مجموعه جمله از دادگان 
\trans{\validation{}}{Validation}
به فضای نهان برده شده و مجددا بازسازی ‌شده و \bleu{} و \selfbleu{} این مجموعه نسبت به داده آزمون اندازه‌گیری و گزارش شده است. این حالت را حالت بازسازی می‌نامیم. در حالت دوم تعدادی نمونه از \priordist{} گرفته شده و توسط \decoder{}  
\decode{}
شده و مجددا معیارهای ذکر شده گزارش شده‌اند. این حالت را نیز حالت نمونه‌برداری می‌نامیم.
مقادیر \bleu{} و \selfbleu{} دادگان \validation{} بر روی دادگان آزمون نیز به شرح زیر است:
عکس بلو سلف بلو :)
اعداد بلو سلف بلوی دیتا :)
\\
همان طور که در تصاویر ؟؟؟ مشخص است، با اینکه مقدار \bleu{} از دادگان اصلی بیشتر است (!) اما \selfbleu{} نزدیک به یک است! این موضوع نشان از این دارد که به ازای تغییرات $z$ در فضای نهان، خروجی \decoder{} تغییر چندانی نمی‌کند. مشاهده نمونه‌های تولید شده توسط مدل نیز گواهی بر این موضوع است.
گواه موضوع :)
\\
حدس اولیه بر این بود که اتفاقی شبیه به آنچه در \vae{} گزارش شده است، رخ داده است. به منظور آزمایش دقیق‌تر یک \autoencoder{} ساده و بدون هیچ تابع هزینه اضافی آموزش داده شد. نتایج به شرح زیر بدست آمد:
نتایج به شرح :)
\\
به وضوح مشخص است که پدیده مشابهی رخ داده است. بنابراین مشکل احتمالا از نحوه آموزش نیست. در گام بعدی تغییر معماری در \encoder{} و \decoder{} مورد بررسی قرار گرفت. معماری‌های مورد آزمایش \lr{LSTM}، \lr{CNN} و \lr{Transformer}  بودند. از آنجا که قدرت \lr{CNN} در تولید جمله در مقایسه با \lr{LSTM} کمتر است، از بررسی \lr{CNN} به عنوان \decoder{} صرف نظر شد. لازم به ذکر است که به دلیل اعمال نکردن توزیعی بر فضای نهان، اعداد برای حالت بازسازی گزارش شده‌اند.
عکس معماری‌های متفاوت و ترکیبشان :)
\\
نکته قابل توجه این است که گویا مشکل در معماری \encoder{} نهفته است. در صورت استفاده از معماری \transformer{} مشکل عدم توجه به فضای نهان تقریبا رفع شده و \bleu{} و \selfbleu{} در حالت بازسازی تقریبا برابر با مقادیر این معیار برای دادگان \validation{} است. نکته قابل توجه دیگر این است که معماری \lstm{} و \transformer{} در \decoder{} تفاوت چندانی ایجاد نمی‌کند و تنها مسیر حرکت متفاوتی دارند.
به دلیل همگرایی سریع‌تر معماری \transformer{}، این معماری برای سایر آزمایشات برگزیده شد. در ادامه با استفاده از معماری \transformer{} یک \wae{} آموزش داده شد که نتایج به شرح زیر است:
شرح آزمایش :)
\\
\fi
\subsection{استفاده از  \gan{} به جای \mmd{}}
در بخش‌های متفاوتی صحبت از رفتار \modecollapse{} 
\gan{}
سخن به میان آمد. در اینجا طی یک آزمایش از \lr{WGAN} به جای \mmd{} برای یادگیری فضای نهان یک \autoencoder{} استفاده شد. به عبارت دیگر اگر $F(\epsilon)$ شبکه‌ای باشد که با گرفتن یک نوفه با توزیع گاوسی نرمال، آن را به نمونه‌ای در فضای نهان تبدیل کند و $D(Z)$ یک \critic{} بین نمونه‌های تولید شده توسط $F$ و فضای نهان ساخته شده توسط \encoder{} 
$Q(Z|X)$
باشد، تابع هزینه ذیل استفاده گشت:
\begin{gather}
 \mathcal{L}_\text{WGAN} (F, D)= 
   \expected_{x \sim P_\text{Data}(X), z \sim Q(Z|x)} D(z)
 - \expected_{\epsilon \sim N(\bff{0}, \bff{I}), z \sim F(\epsilon)} D(z)
  \\
\text{\lr{s.t: D is 1-Lipschitz}} \nonumber
\end{gather}
که نسبت به $D$ بیشینه و نسبت به $F$ کمینه می‌گردد. برای برآوردن شرط \lr{1-Lipschitz} بودن $D$ نیز از روش \lr{Gradient Penalty} استفاده شده است. اگر $\bff{z}$ و $\tilde{\bff{z}}$ به ترتیب نمونه‌هایی از $Q$ و $F$ ،
 $t$
  متغیر تصادفی از توزیع یکنواخت $[0,1]$ و 
  $\hat{z} = t\bff{z} + (1 - t) \tilde{\bff{z}}$
  باشد، \lr{Gradient Penalty} به صورت زیر نوشته می‌شود:
\begin{gather}
\mathcal{L}_\text{WGAN-GP} (D; \bff{z}, \tilde{\bff{z}}) = \lambda_{gp} \expected_{t \sim U(0, 1)}
(|| \nabla_{\hat{\bff{z}}} D(\hat{\bff{z}}) ||_2  - 1)^2
\end{gather}
که در کنار تابع هزینه اصلی بهینه‌سازی می‌شود. نتایج آزمایش فوق به شرح زیر است:
شرح آزمایش :)
\\

\section{آموزش مولد شرطی}

\section{نتایج و مقایسه با سایر مدل‌ها}
از بین مدل‌های شرطی ذکر شده، مدل‌های ساده به دلیل سادگی حذف و مدل \lr{CSGAN} نیز به دلیل ارائه نکردن کد پیاده‌سازی حذف شدند. بنابراین مدل‌های پایه شامل دو مدل \towardctg{} و \sentigan{} هستند اولی بر پایه \vae{} و دیگری بر پایه \gan{} است. در تمامی آموزش‌ها و برای تمامی مدل‌ها، اندازه بردار \embedding{} کلمات و فضای نهان مدل‌ها ۱۲۸ و بردار فضای نهان خروجی \encoder{}، ۵۱۲ در نظر گرفته شده است. در مورد معماری \encoder{} و \decoder{} نیز از معماری \transformer{} استفاده شده است.
\\
همان طور که در بخش ؟؟؟ توضیح داده شد، مدل‌ها بر روی دادگان آموزشی ؟؟؟ آموزش داده شده و سپس بر اساس معیارهای  دقت دسته‌بندی جملات شرطی تولید شده، \bleu{} ، \selfbleu{} و \jaccard{} مورد ارزیابی قرار گرفته‌اند. نتایج در جدول \ref{table:mr15_result} گزارش شده است.
\begin{table*}[!htb]
    \centering
    \caption{ارزیابی مدل‌های پایه و ارائه شده بر اساس معیار‌های مختلف}\label{table:mr15_result}
    \small\tabcolsep=0.07cm
    \begin{tabular}{||c||c c c|c c|c c|c c||}\hline\hline نام مدل	& AC0	& AC1	& Total AC	& BL2	& BL5	& SBL2	& SBL5	& JAC2	& JAC5\\
        \hline\hline
        wae	& $\cellcolor{gray!25}0.661$	& $0.357$	& $0.509$	& $\cellcolor{gray!25}0.588$	& $0.103$	& $\cellcolor{gray!25}0.766$	& $\cellcolor{gray!25}0.180$	& $0.246$	& $0.028$ \\
        \hline
        senti	& $0.655$	& $\cellcolor{gray!25}0.547$	& $\cellcolor{gray!25}0.601$	& $0.583$	& $\cellcolor{gray!25}0.155$	& $0.799$	& $0.587$	& $0.228$	& $\cellcolor{gray!25}0.035$ \\
        \hline
        toward	& $0.559$	& $0.459$	& $0.509$	& $0.513$	& $0.106$	& $0.772$	& $0.479$	& $\cellcolor{gray!25}0.251$	& $0.035$ \\
        \hline
        \hline\end{tabular}\normalsize 
\end{table*}
همان طور که در جدول \ref{table:mr15_result} مشخص است، مدل ارائه شده در دقت یکی از دسته‌ها از سایرین بهتر عمل کرده است؛ این در حالیست که در دقت دسته دیگر، عملکرد ضعیف و در دقت کلی مانند روش \towardctg{} بوده است. در مورد \bleu{} و \selfbleu{} نیز در ۳ مورد از ۴ مورد عملکرد بهتری داشته است. برای مثلا در مورد \bleu[-5]{} و \selfbleu[-5]{}، با اینکه \bleu{} تقریبا نزدیک به سایرین نگه داشته شده است اما \selfbleu{}ی آن عدد پایینی داشته و تنوع بیشتری دارد. لازم به ذکر است که علی‌رقم این موضوع، در معیار \jaccard{} که ترکیب کیفیت و تنوع را در نظر می‌گیرد، عملکرد \jaccard[-2]{}ی نزدیک به بهترین مدل از این نظر را داشته اما در \jaccard[-5]{} با فاصله از سایر مدل‌ها قرار گرفته است.

\subsubsection{بررسی رفتار مولد شرطی}
به منظور بررسی رفتار مولد شرطی، یک آزمایش ترتیب داده شد. همان طور که پیش‌تر توضیح داده شد، علت تقسیم کردن فضای نهان با توجه به مقادیر مختلف شرط، کم بودن ظرفیت مولد شرطی و ضعف آن در یادگیری هر توزیع پیچیده‌ای بود. در اینجا نیز عملکرد نسبتا مناسب آن در یک دسته و عکس آن در دسته دیگر می‌تواند احتمالا به این موضوع مرتبط باشد. به این منظور در تابع هزینه آموزش مولد شرطی تغییر کوچکی اعمال شد؛ به جای اینکه احتمال هر نمونه با شرط مرتبطش را در مولد شرطی بیشینه کنیم، احتمال همان نقاط اما با مقدار شرط دیگری را کمینه میکنیم. در واقع از نقاط با برچسب شرط نادرست به عنوان نمونه منفی بهره گرفته می‌شود. به صورت صوری تابع هزینه ذیل به تابع هزینه اصلی افزوده می‌شود:
\begin{align}
\mathcal{L'}_c (F) = & \lambda_\text{neg} \expected_{c' \sim P_{C'\neq c}, x \sim P_\text{Data}(X|c), z \sim Q(Z|x)} [\log F(z|c')]
\end{align}
و تابع هزینه کلی به صورت زیر بدست می‌آید:
\begin{align}
\mathcal{L}_c (F) = & - \expected_{x \sim P_\text{Data}(X|c), z \sim Q(Z|x)} [\log F(z|c)] +\\
& \lambda_\text{neg} \expected_{c' \sim P_{C'\neq c}, x \sim P_\text{Data}(X|c), z \sim Q(Z|x)} [\log F(z|c')]
\end{align}
پس از آموزش مجدد شبکه با تابع هزینه فوق، خروجی مدل تحت ارزیابی قرار گرفته و نتایج چندان تغییری نکردند که در ذیل آمده است.

از سوی دیگر نیز رفتار زیر در نحوه کمینه و بیشینه شدن به ترتیب احتمال نمونه‌های منفی و مثبت از تابع هزینه مشاهده شد.

در واقع این طور به نظر می‌رسد که مدل مولد شرطی توانایی انتساب احتمال بالا به نمونه‌های مثبت و احتمال کم به نمونه‌های منفی را ندارد و یا هر دو کاهش می‌یابند و یا هر دو افزایش. از این دو مشاهده این طور می‌توان نتیجه‌گیری نمود که اعمال محدودیت تقسیم شده فضای نهان به مقادیر مختلف شرط، چندان کارساز نبوده و همچنان مولد شرطی توانایی تفکیک این دو دسته از یکدیگر را نداشته و احتمالا رفتار \meanseeking{} از خود بروز می‌دهد.
\\
به منظور مشاهده پراکندگی جملات در فضای نهان نیز می‌توان از تصویر سازی جملات در این فضا می‌توان بهره گرفت. به این منظور، از الگوریتم \lr{TSNE} و برچسب مقدار شرط به عنوان رنگ هر نمونه استفاده شده است.


