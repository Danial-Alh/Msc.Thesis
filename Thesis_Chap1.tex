\chapter{مقدمه}\label{Chap:Chap1}
\minitoc
همواره زبان به عنوان یکی از پیچیده‌ترین توانایی‌های بشر مورد توجه بوده است. امروزه نیز همچنان توجه بسیاری از محققان به این مسئله است به طوری که حوزه مستقلی در هوش مصنوعی، به نام پردازش زبان طبیعی را به خود اختصاص داده است. پردازش زبان طبیعی شامل 
\trans{\task{}}{Task}های
مختلفی مانند، 
\trans{تحلیل تمایل}{Semantic Analysis},
\trans{شناسایی موجودیت‌های اسمی}{Named Entity Recognition},
\trans{ترجمه}{Translation},
\trans{پرسش و پاسخ}{Question Answering}
و بسیاری دیگر است. در این بین \task{}
\trans{مدل زبانی}{Language Modeling}
از اهمیت خاصی برخوردار است. اول آنکه با مدل مولد مواجه بوده و علاوه بر این بسیاری از پیچیدگی‌های \task{}های دیگر نیز در آن دخیل می‌شود.
مدل‌های زبانی ارائه شده را می‌توان به دو دسته کلی تقسیم نمود. روش‌های
 \trans{نمادین}{Symbolic}
  و روش‌های آماری. روش‌های نمادین، روش‌هایی قدیمی‌تر بوده و بیشتر بر اساس قوانین از پیش تعریف شده کار می‌کنند. این قوانین دقت بالایی در رعایت قوانین دستور زبان دارند اما
   \trans{فراخوانی}{Recall}
   پایینی دارند؛ چرا که این قوانین تمامی پیچیدگی‌های زبان را مدل نمی‌کنند.\\
در مقابل روش‌های آماری قرار دارند که پایه آن‌ها مبنی بر پیش‌بینی کلمه بعدی با داشتن کلمه‌های پیشین است که این سبک مدل‌سازی مسئله به مدل‌های \trans{خودبرگشتی}{Auto-regressive} معروف هستند. \\
در دهه اخیر با ظهور 
\trans{\gpu}{Graphics Processing Unit (GPU)}
و افزایش قدرت محاسباتی ماشین‌ها، امکان آموزش و یادگیری پارامترهای توابع غیر خطی با تعداد لایه‌های زیاد امکان پذیر شده و حوزه‌ای به نام یادگیری ژرف به وجود آمده است؛ برای مثال اخیرا مدل‌هایی با صد میلیون، سیصد میلیون و حتی یک و نیم میلیارد پارامتر آموزش داده و منتشر شده‌اند. در واقع در هر جایی که به دنبال یادگیری تابعی پیچیده هستیم، می‌توان از یادگیری ژرف بهره برد. به دلیل کاربرد بیشتر و ذات پیوسته تصاویر، بیشتر مدل‌های معرفی شده بر روی تصاویر مورد آزمایش قرار می‌گرفتند. در مقابل به دلیل ذات گسسته متون، حوزه پردازش متن کمی دیرتر در این زمینه رونق گرفت. اما اکنون مدل‌هایی مخصوص پردازش داده‌های دارای حالت دنباله همچون \lstm{} و \transformer{} معرفی شده‌اند که ضمن مدیریت تعداد پارامتر‌ها جملات را به عنوان ورودی دریافت کرده و خروجی متناظر \task{}های مختلف را تولید می‌کنند. در واقع می‌توان هر کلمه را یک متغیر تصادفی در نظر گرفت که مقدار آن نمایانگر کلمه جاری و معمولا تابعی از کلمات پیشین است. روش‌های مورد استفاده برای تولید متن و مدل زبانی، تنها محدود به کاربرد در این حوزه نبوده و در حوزه‌های دیگری همچون تولید گراف، موسیقی، مولکول و هر وظیفه دیگری که حالت دنباله دارد، دارای کاربرد است.\\
اگر بخواهیم مقداری \task{} مدل زبانی را پیشرفته‌تر کنیم، می‌توان این انتظار را داشت که خروجی مدل را کنترل کرد. برای مثال جمله‌ای تولید شود که از کلمات مشخصی تشکیل شده باشد؛ نظری مثبت راجع به یک کالای خاص تولید کند که به این \task{}، تولید شرطی متن گفته می‌شود. تولید شرطی متن می‌تواند حالات بسیاری را شامل شود؛ از تغییر یک ویژگی در جمله؛ برای مثال تبدیل یک نظر مثبت به منفی تا ترجمه یک جمله از یک زبان به زبان دیگر، همگی شامل مدل‌های مولدی هستند که مشروط به یک مقدار هستند. در مورد اول مشروط به حالت اولیه جمله و حالت مقصود دوم و درمورد ترجمه نیز مشروط به جمله زبان اولیه است. حتی مدل‌های  مولد بر پایه فضای نهان را نیز می‌توان به عنوان مدل‌های شرطی در نظر گرفت. چرا که جملات تولید شده تابعی از فضای نهان هستند. در اینجا برای مشخص کردن محدوده پروژه، 
{\bf
    مقصود از تولید شرطی، تولید جملات به شرط مقادیر گسسته و محدود است؛ برای مثال تولید یک جمله در مورد سیاست/ورزش.}
\section{تعریف مساله}
همان طور که پیش‌تر توضیح داده شد، هر کلمه یک متغیر تصادفی است که هر مقدار آن متناظر با یک کلمه است. یک مدل زبانی غیر شرطی \autoregressive{} را می‌توان به صورت زیر مدل کرد.\\
اگر $X_t$ متغیر تصادفی متناظر با کلمه $t$ام و طول جمله $T$ باشد، طبق قانون زنجیره‌ای خواهیم داشت:
\begin{equation}\begin{split}
P_\theta(X_1, X_2, ... , X_T) = P_\theta(X_1) P_\theta(X_2|X_1) P_\theta(X_3|X_2, X_1) ... P_\theta(X_T|X_1, ..., X_{T-1})
\end{split}\end{equation}
که در واقع به دنبال مدل کردن $P_\theta(X_t|X_1, ..., X_{t-1 })$ و یافتن پارامتر‌های $\theta$ هستیم. نمونه برداری از این مدل‌ها نیز به این صورت است که ابتدا کلمه اول از توزیع $P_\theta(X_1)$ نمونه‌برداری شده و سپس کلمه دوم از توزیع $P_\theta(X_2|X_1)$ و سایر کلمات به ترتیب نمونه‌برداری می‌شوند؛ واضح است که به دلیل سبک مدل‌سازی کلمات به صورت ترتیبی بایستی تولید شوند. \\
نسخه دیگری از تولید دنباله نیز تحت عنوان غیر \autoregressive{} وجود دارد که در مدل‌های زبانی چندان اقبالی نداشته‌اند. این مدل‌ها به این صورت هستند که هر کلمه تابعی از کلمات پیشین خود نیست و تمام کلمات به صورت موازی تولید می‌شوند. برای مثال در کاربرد ترجمه می‌توان فرمول‌بندی زیر را در نظر گرفت:
\begin{align}
P_\theta(X_1, X_2, ... , X_T|Y) = P_\theta(X_1) P_\theta(X_2|Y) P_\theta(X_3|Y) ... P_\theta(X_T|Y).
\end{align}
که $Y$ جمله زبان مبدا است و کلمات مستقل از یکدیگر و تنها تابعی از جمله مبدأ هستند. مزیت این مدل‌ها در داشتن فاز تولید نمونه سریع‌تر است چرا که کلمه‌ها بر خلاف مدل‌های \autoregressive{} از یکدیگر مستقل هستند و با داشتن $Y$ می‌توان همه را به صورت موازی نمونه‌برداری نمود؛ با این وجود به لحاظ کارایی از مدل‌های \autoregressive{} ضعیف‌ترند. در این پروژه نیز از این دسته از مدل‌ها صرف نظر شده است.
\\
اگر مدل زبانی دارای فضای نهان باشد که با $Z \in \bb{R}^d$ نشان دهیم، مدل زبانی را می‌توان به صورت زیر تعریف کرد:
\begin{align}
P_\theta(X_1, X_2, ... , X_T,Z) =&P(Z) P_\theta(X_1|Z) P_\theta(X_2|X_1,Z) P_\theta(X_3|X_2, X_1,Z) ... \nonumber\\& P_\theta(X_T|X_1, ..., X_{T-1},Z)
\end{align}
و $P(Z)$ به توزیع
\trans{پیشین}{Prior}
شناخته شده و معمولا ثابت است؛ این در حالیست که در بعضی از مدل‌ها این توزیع نیز یاد گرفته می‌شود. برای نمونه‌برداری از این مدل‌ها نیز ابتدا از \priordist{} نمونه‌برداری شده اما بعد از برای نمونه‌برداری کلمات می‌توان پارامتر به نام دما استفاده نمود که مقداری بین صفر و یک داشته و میزان تیز بودن توزیع را کنترل می‌کند و اگر در حالت نزدیک به صفر توزیع را به توزیع $\argmax$ تبدیل می‌کند. این تابع که به \lr{Soft-argmax} معروف است در فصل بعد توضیح داده خواهد شد. بنابراین با داشتن $Z$ می‌توان با پارامتر دما‌های متفاوت از مدل نمونه‌برداری کرد. شاید معقولانه باشد که به دنبال 
$\argmax_{\bff{x_1}, \bff{x_2}, ..., \bff{x_T}} P_\theta(X_1, X_2, ... , X_T|Z)$
باشیم؛ اما یافتن دنباله با احتمال بیشینه در زمان چندجمله‌ای امکان پذیر نبوده و از \greedydecoding{} که همان استفاده از $\argmax$ در هر زمان است، استفاده می‌شود. در بهترین حالت می‌توان از 
\trans{\beamsearch{}}{Beam Search} 
بهره برد که به دلیل کُند بودن چندان در مدل‌های زبانی مورد استفاده قرار نگرفته است.
\\
برای تغییر تعاریف فوق به حالت شرطی تنها بایستی توزیع‌ها مشروط گردند. اگر شرط با $C \in {0,1,2,...}$ نشان داده شود، مدل زبانی شرطی بدون فضای نهان به صورت 
\begin{align}
    P_\theta(X_1, X_2, ... , X_T|C) = P_\theta(X_1|C) P_\theta(X_2|X_1,C) P_\theta(X_3|X_2, X_1,C) ... P_\theta(X_T|X_1, ..., X_{T-1},C)
\end{align}
و با فضای نهان به صورت 
\begin{align}
P_\theta(X_1, X_2, ... , X_T,Z|C) =&P(Z|C) P_\theta(X_1|Z.C) P_\theta(X_2|X_1,Z.C) P_\theta(X_3|X_2, X_1,Z.C) ... \nonumber\\& P_\theta(X_T|X_1, ..., X_{T-1},Z.C)
\end{align}
تعریف می‌شود.
\section{مدل زبانی با فضای نهان یا بدون فضای نهان؟}
مدل‌های زبانی با فضای نهان به این صورت هستند که فرض شده است جملات در فضای نهان ‎کد شده و شبکه ‎\decoder یی با دریافت برداری از فضای نهان، آن را به جمله مربوطه برگردان می‌کند. با داشتن چنین قابلیتی امکان کنترل کردن شبکه ‎\decoder{}‎ وجود داشته و حتی می‌توان با حرکت روی این فضا جملات شبیه به یکدیگر تولید نموده و یا از یک جمله شروع کرده و به مرور با تغییر بردار ورودی (از فضای نهان) آن به جمله دیگری تبدیل کنیم. این در حالیست که مدل‌های زبانی بدون فضای نهان از این قابلیت بی‌بهره بوده و نمی‌توان کنترلی بر نحوه خروجی آن‌ها داشت.
\section{رویکردهای آموزشی}
به طور کلی دو رویکرد برای آموزش مدل‌های زبانی ارائه شده است. مبتنی بر \likelihood{} و 
\trans{\gan{}}{Generative Adversarial Networks}.
در ادامه این دو رویکرد به طور کلی توضیح داده خواهند شد.
\\
{\bf مبتنی بر \likelihood{}}:
این رویکرد که به 
\trans{\maxlikelihood{}}{Maximum Likelihood} 
معروف است، نیاز به محاسبه احتمال جملات در مدل دارد. در حالت مدل بدون فضای نهان، از آنجا که توزیع توام کلمات یک جمله با استفاده از قانون زنجیره‌ای به احتمال شرطی هر کلمه به کلمات قبل خود بدست می‌آید، بنابراین برای به دست آوردن احتمال یک جمله تنها کافیست احتمال هر کلمه به شرط کلمات قبل خود وجود داشته باشد. لازم به ذکر است که به دلیل محدود بودن واژگان این احتمال به راحتی قابل محاسبه است. بنابراین برای بیشینه کردن \likelihood{} یک جمله در مدل تنها کافیست ضمن ورودی دادن جمله به مدل، در هر زمان احتمال کلمه بعدی بیشینه گردد. به این روش آموزش مدل زبانی روش 
\trans{\teacherforcing{}}{Teacher Forcing}
نامیده می‌شود.
\\
در صورتی که مدل دارای فضای نهان باشد، به منظور محاسبه احتمال یک نمونه، نیاز به انتگرال زیر است که به صورت فرم بسته امکان محسابه آن وجود ندارد و عملا بهینه‌سازی آن به صورت مستقیم امکان‌پذیر نیست.
\begin{align}
P_\theta(X_1, X_2, ... , X_T) = \int_z P(\bff{z})P_\theta(X_1, X_2, ... , X_T|\bff{z})
\end{align}
 از این رو، به جای بشینه کردن مستقیم \likelihood{}، ضمن در نظر گرفتن مدلی به عنوان تخمینی از توزیع
 \trans{\posterior{}}{Posterior}
 کمک گرفته و کران پایینی از آن که به \lr{ELBO} شناخته می‌شود بیشینه می‌گردد. این دسته از مدل‌ها 
\trans{\vae{}}{Variational Autoencoder}
نامیده می‌شوند. تفاوت این دسته از مدل‌ها به لحاظ ساختاری با 
\trans{\autoencoder}{Autoencoder}ها
در داشتن توزیع ثابت در فضای نهان هستند و تفاوت دیگر ندارند.
\\
{\bf مبتنی بر \gan{}}:
\section{اهمیت و کاربرد}

\section{چالش‌ها}

\section{هدف پژوهش}


\section{ساختار پایان‌نامه}




