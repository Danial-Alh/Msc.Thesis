\chapter{راهکار پیشنهادی}\label{Chap:Chap3}
\minitoc

\section{مقدمه}
در فصول قبل به بررسی روش‌های پیشین پرداخته شد. این روش‌های شامل رویکرد‌های مختلفی از جمله مدل‌های مبتنی بر \gan{} و یا \vae{} بودند. در ادامه ضمن مروری بر ویژگی‌های کلی مثبت و منفی این دسته از مدل‌ها سعی بر ارائه مدلی است که مزایای هر دو دسته را در بر داشته باشد.
\\
برای شرطی کردن یک مدل زبانی راهکارهای متفاوتی ارائه شد. در ساده‌ترین حالت می‌توان با وارد کردن شرط به یک شبکه \lr{RNN} به یک مدل شرطی دست یافت. این مدل را از دو جهت می‌توان مورد بررسی قرار داد. اول اینکه به دلیل استفاده از روش \teacherforcing{} در آموزش آن، مدل دارای مشکل \expbias{} است. مشکل دوم که شاید نتوان بر روی آن نام مشکل نهاد، مربوط به عدم داشتن فضای نهان است؛ به عبارت دیگر همان طور که در بخش ؟؟؟ توضیح داده شد، این دسته از مدل‌ها غیر از مقدار شرط که در ورودی دریافت می‌کنند، ورودی دیگری ندارند و تنها می‌توان با نمونه‌گیری از توزیع نهایی آن‌ها، به نمونه‌های مورد نظر دست یافت. این در حالیست که اگر مدل دارای فضای نهان می‌بود، این امکان وجود داشت تا با حرکت در اطراف یک نقطه از فضای نهان، خروجی مدل را کنترل کرد.
\\
دسته بعدی از مدل‌ها را می‌توان مدل‌های مبتنی بر \gan{} در نظر گرفت. با وجود اینکه مزیت ویژه این دسته از مدل‌ها در داشتن  نمونه‌های با کیفیت است اما از ضعف‌های حائز اهمیتی همچون \modecollapse{} رنج می‌برد. این شرایط در حالیست که به دلیل گسسته بودن فضای جملات، در صورت تمایل به استفاده از \gan{} در انتقال گرادیان نیز مشکل اساسی وجود خواهد داشت و که خود این روش‌ها یا روش‌های تقریبی بوده و در غیر این صورت واریانس آموزشی بالایی دارند. لازم به ذکر است اکثر مدل‌های مبتنی بر \gan{} در حوزه متن نیز بدون فضای نهان هستند. در صورت داشتن فضای نهان نیز از آنجا که امکان رخداد \modecollapse{} وجود دارد، فضای نهان به تعداد محدودی جمله نگاشت شده و عملا نمی‌توان از آن انتظار تفسیرپذیری چندانی داشت.
\\
دسته آخر نیز مربوط به مدل‌های \vae{} است. این مدل‌ها از این جهت که هم دارای فضای نهان با توزیع مشخص هستند و با معیار \likelihood{} آموزش داده می‌شوند، بنابراین برخلاف \gan{} دچار مشکل \modecollapse{} نبوده و می‌توان با نمونه‌برداری از فضای نهان (مانند \gan{}) به جملات تبدیل کرد. این مدل‌ها نیز بدون ضعف نیستند. همان طور که در بخش ؟؟؟ توضیح داده شد، مشکل اساسی \vae{} در تولید متن، عدم توجه به فضای نهان است. راه حل‌های متعددی برای این مشکل ارائه شده است که بعضی آن را ناشی از مشکلات تابع هزینه می‌دانند. علاوه بر این، به دلیل استفاده از روش ‌\teacherforcing{}، این مدل نیز دچار \expbias{} است. گذشته از این ضعف‌ها مشکلات دیگری نیز برای آموزش دادن شرطی این مدل‌ها گزارش شده است؛ مانند این موضوع که مدل به طور همزمان توانایی داشتن خطای بازسازی کم و هم رعایت شرط ورودی را چندان خوب ندارد. احتمالا به همین دلیل است که در ؟؟؟ از یک دسته‌بند شرط برای اجبار \decoder{} به رعایت شرط استفاده شده است.

در مجموع هیچ کدام از روش‌های فوق خالی از ضعف نیستند اما از آنجا که هدف در این پروژه تولید جملات به صورت شرطی است، اگر بتوان فضای نهانی با قابلیت تفسیرپذیری بالا به وجود آورد، این امکان وجود دارد تا در صورت لزوم با یادگیری هر بخش از فضای نهان به یک شرط خاص دست یافت. به عبارت دیگر با یک بار یادگیری فضای نهان، می‌توان از آن برای یادگیری شروط مختلف بهره برد و از آنجا که معمولا داده برچسب خورده برای هر وظیفه‌ای به تعداد زیاد موجود نیست، بنابراین می‌توان انتظار داشت با توجه به کوچکتر بودن فضای نهان نسبت به فضای ورودی، با داشتن فضای نهان تفسیرپذیر، وظایف مربوط به شروط مختلف را حتی با داده کمتری فرا گرفت.
\\
بنابراین مدل نهایی از دو بخش کلی تشکیل شده است:
\begin{itemize}
	\item \autoencoder{}:
	      این بخش وظیفه ساختن فضای نهان را بر عهده دارد. در این بخش سعی بر این است تا ضمن یادگیری یک مدل مولد جمله، فضای نهان تفسیرپذیر نیز یاد گرفته شود. همان‌طور که ییش‌تر توضیح داده شد، \vae{} در یادگیری همزمان بازسازی جمله رو عایت شرط چندان موفق نیست و حذف عامل شرطی از آموزش \decoder{} می‌تواند باعث رفع این دغدغه گردد.
	\item
	      مولد شرطی: این بخش وظیفه یادگیری یک زیرفضا از فضای نهان که مربوط به شروط مورد نظر است را بر عهده دارد.
\end{itemize}
در ادامه به شرح جزئیات این دو بخش پرداخته خواهد شد.

\section{آموزش \autoencoder{}}
این طور بیان شد که هدف اولیه ساختن یک فضای نهان معنادار است. در طراحی چنین مدلی باید به سوالات زیر پاسخ داد:
\begin{itemize}
	\item
	      آیا لازم است تا مدلی از خانواده \vae{}  انتخاب شود؟ توزیع حاکم بر فضای نهان چه مزیتی دارد؟
	\item
	      از چه معماری در \encoder{} و \decoder{} بهره برد؟
\end{itemize}
برای پاسخ به سوالات فوق لازم است تا هدف و ویژگی‌های مدل‌ها مرور شود. در واقع هدف از داشتن فضای نهان، قابلیت کنترل بر روی فضای نهان و خروجی مدل بود. یکی از این اعمال حرکت بر روی فضای نهان است. حال اگر توزیع به فرض گاوسی بر روی فضای نهان حاکم نباشد، همان طور که در مقاله ؟؟؟ توضیح داده شد، در \autoencoder{}، نقاط به فواصل دوری از هم نگاشت شده و عملا فضا حفره حفره شده و حرکت بر روی آن خروجی مطلوبی نخواهد داشت. علاوه بر این جمع کردن فضای نهان جملات در یک توزیع، به دلیل فشرده بودن توزیع هدف، می‌تواند منجر به نزدیک شدن فضای نهان جملات مشابه به یکدیگر شده؛ این در حالیست که همان طور که توضیح داده شد در   \autoencoder{} نقاط نزدیک به هم نگاشت نخواهند شد.
\\
با توجه به دلایل فوق انتخاب یک \autoencoder{} با حاکمیت یک توزیع از پیش تعیین شده بر فضای نهان مبرم به نظر می‌رسد. همان طور که پیشتر اشاره شد \vae{} مشکل عدم توجه به فضای نهان دارد. به طور کلی راه‌حل‌های ارائه شده برای این مشکل را این طور  می‌توان جمع‌بندی کرد که با وجود اینکه تابع هزینه مورد استفاده در این شبکه‌ها، کران بالایی از منفی لگاریتم \likelihood است، اما به طور کلی می‌توان آن را شامل دو بخش بازسازی و منظم‌ساز در نظر گرفت. همان‌طور که در آموزش یک چندجمله‌ای که از تابع منظم‌ساز جمع مجذور وزن‌های تابع استفاده می‌شود، باید میزان توجه و غلبه تابع منظم‌ساز را کنترل کرد، در این مدل‌ها نیز بایستی غلبه بخش \lr{KL} بر بخش بازسازی را کنترل کرد؛ در نتیجه آموزش شبکه سخت خواهد شد. در مقابل یکی از راه‌حل‌های ارائه شده به نام \wae{} با تغییر کوچک تابع هزینه، بخش \lr{KL} را که بین \priordist{} و \posteriordist{} تعریف شده است را به \lr{KL} بین \priordist{} و توزیع حاشیه‌ای \posterior{} تبدیل می‌کند. در نتیجه‌ی این تغییر اثبات می‌کند که تابع هزینه جدید برابر با فاصله \wasser{} بین توزیع داده اصلی و توزیع یادگرفته شده توسط مدل است. توضیح مفصل‌تر این \autoencoder{} و مزیت شهودی آن نسبت به \vae{} در بخش ؟؟؟ آورده شده است. بنابراین پاسخ سوال اول مثبت بوده و از \wae{} استفاده خواهد شد.
در مورد سوال بعدی نیز با آزمایش‌های مختلف کارایی معماری‌های مختلف اندازه‌گیری و گزارش شده است که در فصل ؟؟؟ ارائه خواهند شد.
\\
به منظور تبیین صوری تابع هدف، اگر $‎Q‎$  و $G$ به ترتیب توابع \encoder{} و \decoder{} باشند، خواهیم داشت:
\begin{gather}
	\mathcal{L}_{\text{WAE}}(Q, G) = \expected_{x \sim P_{\text{Data}}(X)}\expected_{z \sim Q(z|x)} [c(x, G(z))] + \lambda . D_ Z(Q_Z, P_Z)
\end{gather}
که $c(.,.)$ یک تابع فاصله در فضای ورودی و  $D_Z(. , .)$ هم هر فاصله‌ای بین دو توزیع $Q_Z$ و $P_Z$ است. $Q_Z$ نیز توزیع حاشیه‌ای حاصل از \encoder{} است؛ به عبارت دیگر:
\begin{gather}
	Q_Z(z) = \sum_x P_{\text{Data}}(x) Q(z|x)
\end{gather}
در مقاله اصلی، دو گزینه استفاده از \gan{} و فاصله \mmd{} به عنوان $D_Z(.,.)$ پیشنهاد شده است که مجددا به دلیل امکان رخداد \modecollapse{} از \mmd{} بهره برده خواهد شد.
اما به جای $c(.,.)$ نیز بایستی از تابع هزینه مناسبی استفاده نمود. بنابر مقاله توضیح داده شده در بخش ؟؟؟، اگر تابع $P_\psi$ تابعی از فضای نهان $Z$ به فضای احتمال جملات با حداکثر طول مشخص
(فضای خروجی $P_\psi$ یک
\trans{\simplex{}}{Simplex}
$|V|^m-1$
بعدی است که $m$ حداکثر طول جملات و $|V|$ نیز اندازه واژگان است) و
$G(Z) = \argmax_{x} P_\psi(x|Z)$
، استفاده از تابع هزینه
$\text{cross-entropy}(P_\psi(X|z), x)$
به جای $c(.,.)$،
$\mathcal{L}_{\text{WAE}}(Q, G)$
کران بالایی برای فاصله \wasser{} بین توزیع داده‌ها و خروجی مدل است.
بنابراین تابع هزینه نهایی به شکل زیر است:
\begin{gather}
	\mathcal{L}_{\text{WAE}}(Q, G) = \expected_{x \sim P_{\text{Data}}(X)}\expected_{z \sim Q(Z|X)} [-\log P_\phi(x|z)] + \lambda . D_ Z(Q_Z, P_Z)
\end{gather}
تا به اینجا همچنان مشکل \expbias{} وجود دارد. همان طور که توضیح داده شد، توزیع $Q_Z$ حاصل از حاشیه‌سازی با $P_\text{Data}$ است و به \priordist{} نزدیک می‌شود. حال اگر $Q_Z$ از حاشیه‌سازی  با هر توزیعی غیر از $P_\text{Data}$ بدست آید، دیگر نزدیک به \priordist{} نخواهد بود. با استفاده از این موضوع می‌توان تابع هزینه دیگری نیز برای تقلیل معضل \expbias{} ارائه داد. اگر خروجی \decoder{} به هر دلیلی از توزیع $P_\text{Data}$ فاصله داشته باشد، توزیع $Q_Z$ حاصل از حاشیه‌سازی با ?? نزدیک به \priordist{} نیست و \decoder{} بایستی سعی در تولید جملاتی داشته باشد تا مجموعه آن‌ها در فضای نهان توزیعی نزدیک به \priordist{} را بسازند. این تابع هزینه را به شکل زیر می‌توان صورت‌بندی کرد:
\begin{gather}
	\mathcal{L}_\text{ExpBias}(G) = \lambda_{e} . D_ Z(Q_Z^G, P_Z)
	\\
	Q_Z^G (z) = \int_{z'} P_Z(z') Q(z|G(z'))
\end{gather}
از آنجا که در روابط هزینه فوق از تابع $\argmax$ استفاده شده است، برای انتقال گرادیان از تقریب \lr{Soft-argmax} بهره برده خواهد شد.  \lr{Soft-argmax} به طور کلی پارامتری به نام $\tau$ دارد که نوع \lr{Straight Through} از به صورت زیر تعریف می‌شود:
\begin{gather}
	\bff{y} = \text{softmax}(\bff{o}/\tau)
	\\
	\bff{u} = \bff{y} - \text{stop-gradient}(\bff{y}) + \text{onehot}(\argmax_{o_i} \bff{o})
\end{gather}
که $\bff{o}$ بردار قبل از اعمال \lr{softmax}
،
$\bff{u}$
بردار خروجی و توابع \lr{onehot} و \lr{stop-gradient} به ترتیب تبدیل‌کننده شاخص به بردار 
\trans{\onehot{}}{Onehot}
و قطع‌کننده عبور گرادیان هستند. بنابراین همان طور که از تعریف برداشت می‌شود، در فاز
\trans{\forward{}}{Forward}
هیچ تقریبی وجود ندارد اما در فاز
\trans{\backward{}}{Backward}
گرادیان از طریق تقریب \lr{Soft-argmax}
($\bff{y}$)
منتقل می‌شود.
به منظور آموزش \encoder{} در تفاوت قائل شدن بین توزیع فضای جملات واقعی و فضای جملات تولید شده توسط \decoder{} می‌توان تابع هزینه زیر را نیز در نظر گرفت:
\begin{gather}
	\mathcal{L}_\text{ExpBias}(Q) = -\lambda_{e} . D_ Z(Q_Z^G, P_Z)
\end{gather}

\section{آموزش مولد شرطی}
تا به اینجا در مورد نحوه ساختن یک مدل مولد غیر شرطی اما با فضای نهان مناسب صحبت شد. حال با داشتن فضای نهان بایستی مدل مولد شرطی را آموزش داد تا با گرفتن مقدار شرط، توزیعی بر روی فضای نهان دارای شرط مورد نظر بسازد.
\\
از آنجا که فضای نهان، یک فضای پیوسته است، بنابراین بایستی کمی در مورد نحوه انتخاب مدل مولد و روش آموزش آن تامل کرد. همان‌طور که پیشتر توضیح داده شد، موفق‌ترین مدل‌های مولد، \gan{} و \vae{} هستند. در مورد \gan{} که مشکل \modecollapse{} وجود دارد و در مورد \vae{} نیز از آنجا که فضای ورودی آن فضای نهان جملات است، بایستی برای این فضا، فضای نهان دیگری را بسازد. بنابراین این روش‌ها چندان معقول نیستند. در کنار این روش‌ها رویکرد دیگری نیز وجود دارد که چندان به آن توجه نمی‌شود. یادگیری مدل مولد بر پایه بیشینه کردن مستقیم \likelihood{}. از آنجا که در حال صحبت از فضای پیوسته هستیم، بنابراین بایستی از توزیع‌های پارامتری از پیش تعریف شده مانند گاوسی و یا در بهترین حالت از
\trans{\gaussianmix{}}{Gaussian mixture}
استفاده کرد. از آنجا که فرضی بر توزیع فضای نهان یک شرط خاص نداریم، بنابراین به کار بستن چنین مدل‌هایی چندان معقول به نظر نمی‌رسد. همان طور که در بخش ؟؟؟‌ توضیح داده شد، نوع دیگری از خانواده‌های مولد وجود دارند که هم امکان محاسبه \likelihood{} در خروجی را دارند و هم فرم مشخصی مانند توزیع گاوسی ندارند. به این دسته از شبکه‌ها \normalizingflownets{} می‌گویند. توضیحات مفصلی در رابطه با معرفی این مدل‌ها در بخش ؟؟؟ ارائه شده است و در اینجا صرف نظر می‌شود.
\\
فرض کنید مدل مولد شرطی را با $F(Z|C)$ که یک شبکه از خانواده \normalizingflownets{} است، نشان داده و مجموعه دادگان آموزشی
$\mathcal{X}_C = \{(\bff{x}_i, \bff{c}_i)\}_{i=1}^{N}$
باشد. اگر
\begin{gather}
	Q(Z, X|C) = P_\text{Data}(X|C) Q(Z|X)
	\\
	Q(Z|c) = \sum_x P_\text{Data}(x|c) Q(Z|x)
\end{gather}
خواهیم داشت:
\begin{align}
	\mathcal{L}_c (F) = & KL\big(Q(Z|c) ~ || ~ F(Z|c)\big) \nonumber                                                    \\
	=                   & \int_z Q(z|c) \log \frac{Q(z|c)}{F(z|c) \nonumber}                                            \\
	=                   & -\int_z Q(z|c) \log F(z|c) + \text{const} \nonumber                                           \\
	=                   & -\int_z \sum_x P_\text{Data}(x|c) Q(z|x) \log F(z|c) + \text{const} \nonumber                 \\
	\Rightarrow
	\mathcal{L}_c (F) = & - \expected_{x \sim P_\text{Data}(X|c), z \sim Q(Z|x)} [\log F(z|c)] + \text{const}
\end{align}
رویکرد‌های متفاوتی نیز برای مدل کردن تابع $F$ وجود دارد؛ اما  همان طور که پیشتر توضیح داده شد، مدل‌های \autoregressive{} از این خانواده به مراتب توانایی مدل‌سازی بیشتری دارند و نقطه ضعف آن‌ها در عدم امکان موازی‌سازی بر روی بُعد است؛ اما از آنجا که ابعاد فضای نهان کوچک است، این نکته چندان مشکل‌ساز نیست. دو نوع ساده از این خانواده \lr{MAF} و  \lr{IAF} است. به دلیل آنکه \lr{IAF} و \lr{MAF} توابع معکوس یکدیگر هستند، \lr{MAF} در ادامه توضیح داده خواهد شد.
\\
اگر تابع $F$ را به صورت $F = F_1 \circ F_2 \circ ... \circ F_L$ باشد که $F_l$ یک لایه از نوع \lr{MAF} است. اگر خروجی لایه $l$ ام را با $Z_l \in \bb{R}^D$ نشان دهیم و $Z_0$ نمونه‌ای از توزیع پایه  $P_{Z_0}$ با قابلیت محاسبه کارای احتمال یک نمونه و همچنین نمونه‌برداری باشد (در اینجا توزیع گاوسی نرمال)، خواهیم داشت:
\begin{align}
    Z^i_{l+1} =& Z^i_{l} * \Theta^{i}_{l,1}(Z^{1:i-1}_{l+1}) + \Theta^{i}_{l,2}(Z^{1:i-1}_{l+1})
\end{align}
و تابع معکوس نیز می‌تواند به صورت برداری به شرح زیر نوشته شود:
\begin{align}
    \bff{Z_{l-1}} =& F_{l}^{-1}(\bff{Z_{l}}) \nonumber
    \\
   \bff{ Z_{l-1}} =& 
   \frac{\bff{ Z_{l}} - \Theta_{l,2}(\bff{Z_{l}})}
               {\Theta_{l,1}(\bff{Z_{l}})}
\end{align}
که $\Theta_{l,1}$ و $\Theta_{l,2}$ نیز توابعی هستند که با شبکه عصبی مدل می‌شوند؛ تنها محدودیت این شبکه‌ها در این است که بُعد $i$ام آن تنها باید تابعی از ابعاد $1$ تا $i-1$ باشد. به این هدف، از معماری \lr{MADE} در $\Theta_{l,1}$ و $\Theta_{l,2}$ استفاده می‌شود.
\\
از آنجا که هر بُعد بر حسب بعدهای قبل از خود است، بنابراین ماتریس ژاکوبین نیز مثلثی بوده و دترمینان آن برابر با حاصل‌ضرب اعضای قطر آن است.
\begin{align}
\Big[ \frac{\partial F_{l+1}}{\partial \bff{Z_l}} \Big]_{i,i} = \Theta^{i}_{l,1}(Z^{1:i-1}_{l+1}) \nonumber
\\
\text{det}\Big[ \frac{\partial F_{l+1}}{\partial\bff{Z_l}} \Big] = \prod_{i=1}^D \Theta^{i}_{l,1}(Z^{1:i-1}_{l+1})
\end{align}
بنابراین \likelihood{} نیز طبق رابطه ؟؟؟ به صورت زیر محاسبه می‌شود:
\begin{align}
\log P_F(Z_L) =& \log P_{Z_0} - \sum_{l=1}^L \log (\text{det}[    \frac{\partial F_{l+1}}{\partial \bff{Z_l}}     ]) \nonumber
\\
=& \log P_{Z_0} - \sum_{l=1}^L \sum_{i=1}^D \log (\Theta^{i}_{l,1}(Z^{1:i-1}_{l+1})) \nonumber
\\
=& \sum_{i=1}^D -\frac{1}{2} \{Z^i_0\}^2 - \sum_{l=1}^L \sum_{i=1}^D \log (\Theta^{i}_{l,1}(Z^{1:i-1}_{l+1})) + \text{const} \nonumber
\\
=& -\sum_{i=1}^D \frac{1}{2} \{Z^i_0\}^2 + \sum_{l=1}^L \log (\Theta^{i}_{l,1}(Z^{1:i-1}_{l+1})) + \text{const}
\end{align}
در نهایت رابطه هزینه $\mathcal{L}_c (F)$ به صورت زیر خواهد بود:

\begin{align}
	\mathcal{L}_c (F) = & - \expected_{x \sim P_\text{Data}(X|c), z \sim Q(Z|x)} [-\sum_{i=1}^D \frac{1}{2} \{z^i_0\}^2 + \sum_{l=1}^L \log (\Theta^{i}_{l,1}(z^{1:i-1}_{l+1}))] + \text{const}
\end{align}
