\chapter{راهکار پیشنهادی}\label{Chap:Chap3}
\minitoc

\section{مقدمه}
در فصول قبل به بررسی روش‌های پیشین پرداخته شد. این روش‌های شامل رویکرد‌های مختلفی از جمله مدل‌های مبتنی بر \gan{} و یا \vae{} بودند. در ادامه ضمن مروری بر ویژگی‌های کلی مثبت و منفی این دسته از مدل‌ها سعی بر ارائه مدلی است که مزایای هر دو دسته را در بر داشته باشد.
\\
برای شرطی کردن یک مدل زبانی راهکارهای متفاوتی ارائه شد. در ساده‌ترین حالت می‌توان با وارد کردن شرط به یک شبکه \lr{RNN} به یک مدل شرطی دست یافت. این مدل را از دو جهت می‌توان مورد بررسی قرار داد. اول اینکه به دلیل استفاده از روش \teacherforcing{} در آموزش آن، مدل دارای مشکل \expbias{} است. مشکل دوم که شاید نتوان بر روی آن نام مشکل نهاد، مربوط به عدم داشتن فضای نهان است؛ به عبارت دیگر همان طور که در بخش ؟؟؟ توضیح داده شد، این دسته از مدل‌ها غیر از مقدار شرط که در ورودی دریافت می‌کنند، ورودی دیگری ندارند و تنها می‌توان با نمونه‌گیری از توزیع نهایی آن‌ها، به نمونه‌های مورد نظر دست یافت. این در حالیست که اگر مدل دارای فضای نهان می‌بود، این امکان وجود داشت تا با حرکت در اطراف یک نقطه از فضای نهان، خروجی مدل را کنترل کرد.
\\
دسته بعدی از مدل‌ها را می‌توان مدل‌های مبتنی بر \gan{} در نظر گرفت. با وجود اینکه مزیت ویژه این دسته از مدل‌ها در داشتن  نمونه‌های با کیفیت است اما از ضعف‌های حائز اهمیتی همچون \modecollapse{} رنج می‌برد. این شرایط در حالیست که به دلیل گسسته بودن فضای جملات، در صورت تمایل به استفاده از \gan{} در انتقال گرادیان نیز مشکل اساسی وجود خواهد داشت و که خود این روش‌ها یا روش‌های تقریبی بوده و در غیر این صورت واریانس آموزشی بالایی دارند. لازم به ذکر است اکثر مدل‌های مبتنی بر \gan{} در حوزه متن نیز بدون فضای نهان هستند. در صورت داشتن فضای نهان نیز از آنجا که امکان رخداد \modecollapse{} وجود دارد، فضای نهان به تعداد محدودی جمله نگاشت شده و عملا نمی‌توان از آن انتظار تفسیرپذیری چندانی داشت.
\\
دسته آخر نیز مربوط به مدل‌های \vae{} است. این مدل‌ها از این جهت که هم دارای فضای نهان با توزیع مشخص هستند و با معیار \likelihood{} آموزش داده می‌شوند، بنابراین برخلاف \gan{} دچار مشکل \modecollapse{} نبوده و می‌توان با نمونه‌برداری از فضای نهان (مانند \gan{}) به جملات تبدیل کرد. این مدل‌ها نیز بدون ضعف نیستند. همان طور که در بخش ؟؟؟ توضیح داده شد، مشکل اساسی \vae{} در تولید متن، عدم توجه به فضای نهان است. راه حل‌های متعددی برای این مشکل ارائه شده است که بعضی آن را ناشی از مشکلات تابع هزینه می‌دانند. علاوه بر این، به دلیل استفاده از روش ‌\teacherforcing{}، این مدل نیز دچار \expbias{} است.

در مجموع هیچ کدام از روش‌های فوق خالی از ضعف نیستند اما از آنجا که هدف در این پروژه تولید جملات به صورت شرطی است، اگر بتوان فضای نهانی با قابلیت تفسیرپذیری بالا به وجود آورد، این امکان وجود دارد تا در صورت لزوم با یادگیری هر بخش از فضای نهان به یک شرط خاص دست یافت. به عبارت دیگر با یک بار یادگیری فضای نهان، می‌توان از آن برای یادگیری شروط مختلف بهره برد و از آنجا که معمولا داده برچسب خورده برای هر وظیفه‌ای به تعداد زیاد موجود نیست، بنابراین می‌توان انتظار داشت با توجه به کوچکتر بودن فضای نهان نسبت به فضای ورودی، با داشتن فضای نهان تفسیرپذیر، وظایف مربوط به شروط مختلف را حتی با داده کمتری فرا گرفت.
\\
بنابراین مدل نهایی از دو بخش کلی تشکیل شده است:
\begin{itemize}
    \item \autoencoder{}:
    این بخش وظیفه ساختن فضای نهان را بر عهده دارد. در این بخش سعی بر این است تا ضمن یادگیری یک مدل مولد جمله، فضای نهان تفسیرپذیر نیز یاد گرفته شود.
    \item
    مولد شرطی: این بخش وظیفه یادگیری یک زیرفضا از فضای نهان که مربوط به شروط مورد نظر است را بر عهده دارد.
\end{itemize}
در ادامه به شرح جزئیات این دو بخش پرداخته خواهد شد.

\section{آموزش \autoencoder{}}
این طور بیان شد که هدف اولیه ساختن یک فضای نهان معنادار است. در طراحی چنین مدلی باید به سوالات زیر پاسخ داد:
\begin{itemize}
    \item
    آیا لازم است تا مدلی از خانواده \vae{}  انتخاب شود؟ توزیع حاکم بر فضای نهان چه مزیتی دارد؟
    \item
    چطور می‌توان تفسیرپذیری فضای نهان را گسترش داد؟
    \item 
    از چه معماری در \encoder{} و \decoder{} بهره برد؟
\end{itemize}
برای پاسخ به سوالات فوق لازم است تا هدف و ویژگی‌های مدل‌ها مرور شود. در واقع هدف از داشتن فضای نهان، قابلیت کنترل بر روی فضای نهان و خروجی مدل بود. یکی از این اعمال حرکت بر روی فضای نهان است. حال اگر توزیع به فرض گاوسی بر روی فضای نهان حاکم نباشد، همان طور که در مقاله ؟؟؟ توضیح داده شد، در \autoencoder{}، نقاط به فواصل دوری از هم نگاشت شده و عملا فضا حفره حفره شده و حرکت بر روی آن خروجی مطلوبی نخواهد داشت. علاوه بر این جمع کردن فضای نهان جملات در یک توزیع، به دلیل فشرده بودن توزیع هدف، می‌تواند منجر به نزدیک شدن فضای نهان جملات مشابه به یکدیگر شده؛ این در حالیست که همان طور که توضیح داده شد در   \autoencoder{} نقاط نزدیک به هم نگاشت نخواهند شد.
\\
با توجه به دلایل فوق انتخاب یک \autoencoder{} با حاکمیت توزیع مشخص بر فضای نهان مبرم به نظر می‌رسد.

\section{آموزش مولد شرطی}
